# 教師あり学習の概要

## 2-1. 導入

## 2-2. 変数の種類と用語
本書では入力変数を$X$と表す。
$X$がベクトルの際、その要素を$X_j$と表す
量的な出力は$Y$、質的（カテゴリカル）な出力は$G$と表す
変数そのものに言及する際は$X$、$Y$、$G$などの大文字を用いて、観測値を表す際には対応する小文字を用いる。例えば、$X$の$i$番目の観測値は$x_i$
$x_i$はスカラーの場合もベクトルの場合もあることに注意
（スカラーは長さや重さなど一つの量だけで表せるもの、ベクトルは速度や位置など向きを伴うもの）
行列は大文字の太字で表す。例えば$N$個の$p$次元入力入力ベクトル$x_i$（$i = 1,....., N$）は、$N\times p$行列$\bf X$と表す

学習とは、入力$X$が与えられたときに出力$Y$の良い予測値$\hat{Y}$を求めることである。
$Y$が実数$\mathbb{R}$上の量的変数ならば$\hat{Y}$も同様に$\mathbb{R}$上の値を取る。質的変数$G$が集合$\mathbb{G}$の意スレ化の値をとれば、その予測値$\hat{G}$も同じ集合$\mathbb{G}$上の値を取る。

## 2-3. 最小二乗法と最近傍法

線形モデルとは
$$\hat{Y} = \hat{\beta_0} + \sum_{j=1}^pX_j\hat{\beta_j}$$
$\hat{\beta_0}$は切片（バイアス）
これを簡潔に表記すると内積で表現できる
$$\hat{Y} = X^{T}\cdot \hat{\beta}$$
（$X$に定数1を追加して、係数ベクトル$\hat{\beta}$に$\hat{\beta_0}$を追加したということである）

$(X, \hat{Y})$は、入力と出力を合わせた$(p+1)$次元入出力空間において、超平面を構成する。（入力と出力を合わせて2次元の場合は$(X, \hat{Y})$は直線となるし、入力と出力を合わせて3次元の場合は$(X, \hat{Y})$は平面となる）

